{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93a07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "from models import SITE\n",
    "\n",
    "# Device configuration\n",
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2d427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model_path = './models/CIFAR'\n",
    "dataset_path = '../data'\n",
    "\n",
    "image_size = 128\n",
    "n_epoch = 100\n",
    "\n",
    "Transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.Resize((image_size,image_size)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                          std = [0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=dataset_path, train=True,\n",
    "                                        download=True, transform=Transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "valset = torchvision.datasets.CIFAR10(root=dataset_path, train=False,\n",
    "                                       download=True, transform=Transform)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=8)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b8db48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are properly built! There are totally 7819592 parameters.\n"
     ]
    }
   ],
   "source": [
    "site = SITE().to(device)\n",
    "print(f'Models are properly built! There are totally {get_n_params(site)} parameters.')\n",
    "\n",
    "optimizer = optim.Adam(site.parameters(), lr = 1e-2)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [50, 80], gamma = 0.5)\n",
    "\n",
    "celoss = nn.CrossEntropyLoss()\n",
    "bceloss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b6feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [640, 10, 16, 16]], which is output 0 of CudnnGridSamplerBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3466384/2635687438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_cls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_rec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [640, 10, 16, 16]], which is output 0 of CudnnGridSamplerBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    Loss_cls = []\n",
    "    Loss_rec = []\n",
    "    equality = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    # generate new prototypes\n",
    "    prototype = get_prototype(train_loader)\n",
    "    site.train()\n",
    "    \n",
    "    for batch_idx, (image, label) in tqdm(enumerate(train_loader)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        image, label = image.to(device), label.to(device)\n",
    "        \n",
    "        theta, gamma = get_theta(image.shape[0], get_reverse = True)\n",
    "        t_image = transform(image, theta, dataset = 'CIFAR')\n",
    "        \n",
    "        t_feature, t_W, t_pred  = site.for_training(t_image)\n",
    "        \n",
    "        # Classification loss\n",
    "        loss_cls = celoss(t_pred, label)\n",
    "        Loss_cls.append(loss_cls.item())\n",
    "\n",
    "        target = sample_prototype(prototype, label, dataset = 'CIFAR')\n",
    "        expanded_theta = theta.view(image.shape[0],1,2,3).expand(image.shape[0],10,2,3).reshape(image.shape[0]*10,2,3)\n",
    "        t_target = transform(target.view(-1, 3, 128, 128), expanded_theta, dataset = 'CIFAR')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            t_target_feature, _ = site.backbone(t_target.view(-1, 3, 128, 128))\n",
    "            t_target_feature = t_target_feature.view(-1, 10, 10, 16, 16)\n",
    "        loss_rec = 5*bceloss((t_W + 1)/2, (t_target_feature + 1)/2)\n",
    "        Loss_rec.append(loss_rec.item())\n",
    "        \n",
    "        loss = loss_cls + loss_rec\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        equality += (t_pred.max(1)[1] == label).float().mean()\n",
    "    \n",
    "    train_accuracy = equality / (batch_idx + 1)\n",
    "    scheduler.step()\n",
    "    \n",
    "    equality = 0\n",
    "    site.eval()\n",
    "    for batch_idx, (image, label) in enumerate(val_loader):\n",
    "        \n",
    "        image, label= image.to(device), label.to(device)\n",
    "        \n",
    "        theta, gamma = get_theta(image.shape[0], get_reverse = True)\n",
    "        t_image = transform(image, theta, dataset = 'CIFAR')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            t_pred = site(image)\n",
    "            \n",
    "        equality += (t_pred.max(1)[1] == label).float().mean()\n",
    "        \n",
    "    val_accuracy = equality / (batch_idx + 1)\n",
    "    \n",
    "    print('epoch: {}, loss: {:.3f}/{:.3f}, train_acc: {:.4f}, val_acc: {:.4f}, time: {:.2f}'.format(\n",
    "        epoch + 1,\n",
    "        torch.FloatTensor(Loss_cls).mean(),\n",
    "        torch.FloatTensor(Loss_rec).mean(),\n",
    "        train_accuracy,\n",
    "        val_accuracy,\n",
    "        time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e7150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
